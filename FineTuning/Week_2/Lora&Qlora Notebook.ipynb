{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tieoj7bSLMc6"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "\n",
        "1.   torch : Le package principal de PyTorch pour les tenseurs.\n",
        "2.   torch.nn : Contient les modules de réseaux neuronaux.\n",
        "3. torch.optim : Fournit des algorithmes d'optimisation.\n",
        "4. torch.nn.functional : Contient des fonctions comme relu et linear, qui sont des versions fonctionnelles des modules.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IMVmuQVGfLy",
        "outputId": "4b1cb822-fc36-411b-94b6-46f99e0e0f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_eCaWEH6JYB_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kZ7OZ8_Lqm9"
      },
      "source": [
        "\n",
        "\n",
        "1. TinyLlama : Un modèle de réseau neuronal simple avec deux couches linéaires (fc1 et fc2).\n",
        "\n",
        "2. fc1 : Une couche linéaire prenant une entrée de dimension 768 et produisant une sortie de dimension 1024.\n",
        "3. fc2 : Une autre couche linéaire qui prend une entrée de 1024 dimensions et produit une sortie de 2 dimensions, représentant les classes de sortie pour la classification binaire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YiS9AusOJQOl"
      },
      "outputs": [],
      "source": [
        "class TinyLlama(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyLlama, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 2)  \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1otQVBRqL7nF"
      },
      "source": [
        "1. LoRALayer : Ajoute des matrices de faible rang (A et B) à une couche linéaire pour adapter le modèle préentraîné à une nouvelle tâche.\n",
        "2. original_layer : La couche linéaire d'origine que nous voulons adapter.\n",
        "3. lora_A et lora_B : Matrices de faible rang qui sont des paramètres entraînables ajoutés pour ajuster la sortie de la couche originale.\n",
        "4. forward : Calcule d'abord la sortie d'origine avec original_layer, puis ajoute l'ajustement calculé à l'aide des matrices lora_A et lora_B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jVjY-Vd-JmCP"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, original_layer, input_dim, r=4):\n",
        "        super(LoRALayer, self).__init__()\n",
        "        self.original_layer = original_layer\n",
        "        self.input_dim = input_dim\n",
        "        self.r = r\n",
        "        self.lora_A = nn.Parameter(torch.randn(input_dim, r))\n",
        "        self.lora_B = nn.Parameter(torch.randn(r, original_layer.weight.size(0)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_output = self.original_layer(x)\n",
        "        lora_output = torch.mm(torch.mm(x, self.lora_A), self.lora_B)\n",
        "        return original_output + lora_output\n",
        "\n",
        "class TinyLlamaWithLoRA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyLlamaWithLoRA, self).__init__()\n",
        "        self.fc1 = LoRALayer(nn.Linear(768, 1024), input_dim=768)\n",
        "        self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJLDIoMFMgMM"
      },
      "source": [
        "1. QLoRALayer : Similaire à LoRALayer, mais inclut une quantification des poids de la couche originale.\n",
        "2. quantized_weight : Les poids de original_layer sont quantifiés, réduisant la précision pour économiser de la mémoire.\n",
        "3. Quantification : Convertit les poids en un format de moindre précision (ex: 4-bit).\n",
        "4. Déquantification et Inférence : Avant de calculer la sortie, les poids sont déquantifiés pour obtenir les valeurs approximatives d'origine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vYIxM3f1KLA2"
      },
      "outputs": [],
      "source": [
        "class QLoRALayer(nn.Module):\n",
        "    def __init__(self, original_layer, input_dim, r=4):\n",
        "        super(QLoRALayer, self).__init__()\n",
        "        self.original_layer = original_layer\n",
        "        self.input_dim = input_dim\n",
        "        self.r = r\n",
        "        self.lora_A = nn.Parameter(torch.randn(input_dim, r))\n",
        "        self.lora_B = nn.Parameter(torch.randn(r, original_layer.weight.size(0)))\n",
        "        # Quantification des poids de la couche originale\n",
        "        self.quantized_weight = torch.quantize_per_tensor(\n",
        "            original_layer.weight.data, scale=0.1, zero_point=0, dtype=torch.qint8\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Déquantification pour l'inférence\n",
        "        dequantized_weight = self.quantized_weight.dequantize()\n",
        "        original_output = F.linear(x, dequantized_weight)\n",
        "        lora_output = torch.mm(torch.mm(x, self.lora_A), self.lora_B)\n",
        "        return original_output + lora_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1ebUlEWHJ3m-"
      },
      "outputs": [],
      "source": [
        "class TinyLlamaWithQLoRA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyLlamaWithQLoRA, self).__init__()\n",
        "        self.fc1 = QLoRALayer(nn.Linear(768, 1024), input_dim=768)\n",
        "        self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def calculate_accuracy(model, inputs, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S_xk8PuMJ4fY"
      },
      "outputs": [],
      "source": [
        "train_inputs = torch.randn(100, 768)\n",
        "train_labels = torch.randint(0, 2, (100,))\n",
        "test_inputs = torch.randn(20, 768)\n",
        "test_labels = torch.randint(0, 2, (20,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f7WWb86NJzDk"
      },
      "outputs": [],
      "source": [
        "model_lora = TinyLlamaWithLoRA()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_lora.parameters(), lr=0.001)\n",
        "model_lora.train()\n",
        "for epoch in range(5):  \n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_lora(train_inputs)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "accuracy_lora = calculate_accuracy(model_lora, test_inputs, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7LZ6ZcLJrsa",
        "outputId": "c9c52ceb-5040-4d88-8aeb-6be2200527a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy LoRA: 45.00%\n",
            "Accuracy QLoRA: 50.00%\n"
          ]
        }
      ],
      "source": [
        "model_qlora = TinyLlamaWithQLoRA()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_qlora.parameters(), lr=0.001)\n",
        "model_qlora.train()\n",
        "for epoch in range(5): \n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_qlora(train_inputs)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "accuracy_qlora = calculate_accuracy(model_qlora, test_inputs, test_labels)\n",
        "\n",
        "print(f\"Accuracy LoRA: {accuracy_lora * 100:.2f}%\")\n",
        "print(f\"Accuracy QLoRA: {accuracy_qlora * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
